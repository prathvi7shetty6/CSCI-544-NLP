{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e3e41ce",
   "metadata": {},
   "source": [
    "### Name: Prathvi Shetty\n",
    "### Python 3.11.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2b481ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "import contractions \n",
    "import re\n",
    "import gensim.downloader as api\n",
    "from gensim.utils import simple_preprocess\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "from sklearn.svm import LinearSVC\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "859f9f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03176485",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "data = pd.read_csv('data.tsv',on_bad_lines='skip', sep=\"\\t\",usecols=['star_rating','review_body'])\n",
    "\n",
    "# Create 2 classes\n",
    "data['class'] = data['star_rating'].apply(lambda x: 1 if x in [1,2,3] else 2)\n",
    "\n",
    "# Select 50,000 reviews randomly for both the classes\n",
    "balanced_data = pd.DataFrame(columns=data.columns)\n",
    "for rating in data['class'].unique():\n",
    "    class_set = data[data['class'] == rating]\n",
    "    random_sample = class_set.sample(n=50000)\n",
    "    balanced_data = pd.concat([balanced_data,random_sample])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14a00eb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>star_rating</th>\n",
       "      <th>review_body</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>595562</th>\n",
       "      <td>5</td>\n",
       "      <td>What's not to like?</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>961291</th>\n",
       "      <td>5</td>\n",
       "      <td>Excellent product at a great price.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2079982</th>\n",
       "      <td>5</td>\n",
       "      <td>Pros:&lt;br /&gt;Cheap ink.&lt;br /&gt;Hassle free experie...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200321</th>\n",
       "      <td>5</td>\n",
       "      <td>Perfect.  We needed these for a trade show and...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1028783</th>\n",
       "      <td>5</td>\n",
       "      <td>Great for the C&amp;C cage i built for my guinea p...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>757222</th>\n",
       "      <td>1</td>\n",
       "      <td>Was Very surprised.&lt;br /&gt;Did lots of research,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>802600</th>\n",
       "      <td>3</td>\n",
       "      <td>Comes with all hardware to mount directly to o...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1813173</th>\n",
       "      <td>2</td>\n",
       "      <td>Great phone for Skype use.  Phone died about 6...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2602978</th>\n",
       "      <td>3</td>\n",
       "      <td>It is missing a critical feature. There is no ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>761848</th>\n",
       "      <td>2</td>\n",
       "      <td>I was very disappointed in this calendar.  My ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        star_rating                                        review_body class\n",
       "595562            5                                What's not to like?     2\n",
       "961291            5                Excellent product at a great price.     2\n",
       "2079982           5  Pros:<br />Cheap ink.<br />Hassle free experie...     2\n",
       "200321            5  Perfect.  We needed these for a trade show and...     2\n",
       "1028783           5  Great for the C&C cage i built for my guinea p...     2\n",
       "...             ...                                                ...   ...\n",
       "757222            1  Was Very surprised.<br />Did lots of research,...     1\n",
       "802600            3  Comes with all hardware to mount directly to o...     1\n",
       "1813173           2  Great phone for Skype use.  Phone died about 6...     1\n",
       "2602978           3  It is missing a critical feature. There is no ...     1\n",
       "761848            2  I was very disappointed in this calendar.  My ...     1\n",
       "\n",
       "[100000 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "balanced_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ebdc4ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting into lower case\n",
    "balanced_data['review_body'] = balanced_data['review_body'].str.lower()\n",
    "# Remove HTML tags\n",
    "balanced_data['review_body'] = balanced_data['review_body'].apply(lambda d: re.sub(r'<.*?>','',str(d)))\n",
    "# Remove URLs\n",
    "balanced_data['review_body'] = balanced_data['review_body'].apply(lambda d: re.sub(r'https?://\\S+www.\\S+', '', d))\n",
    "# Remove non alphabetical character\n",
    "balanced_data['review_body'] = balanced_data['review_body'].apply(lambda d: re.sub(r'[^a-zA-Z\\s]','',d))\n",
    "# Remove extra spaces\n",
    "balanced_data['review_body'] = balanced_data['review_body'].apply(lambda d: ' '.join(d.split()))\n",
    "# Perform contractions\n",
    "balanced_data['review_body'] = balanced_data['review_body'].apply(lambda d: contractions.fix(d))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee48957",
   "metadata": {},
   "source": [
    "### 2. Word Embedding (25 points). In this part the of the assignment, you will generate Word2Vec features for the dataset you generated. You can use Gensim library for this purpose."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97694a06",
   "metadata": {},
   "source": [
    "### a) Load the pretrained “word2vec-google-news-300” Word2Vec model and learn how to extract word embeddings for your dataset. Try to check semantic similarities of the generated vectors using three examples of your own, e.g., King − M an + W oman = Queen or excellent ∼ outstanding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f285c608",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "word2vect_model = api.load(\"word2vec-google-news-300\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5ff850f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using simple_preproces and replacing null values\n",
    "balanced_data = balanced_data.fillna('')\n",
    "balanced_data['token'] = balanced_data['review_body'].apply(lambda x: simple_preprocess(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4384062d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>star_rating</th>\n",
       "      <th>review_body</th>\n",
       "      <th>class</th>\n",
       "      <th>token</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>595562</th>\n",
       "      <td>5</td>\n",
       "      <td>what is not to like</td>\n",
       "      <td>2</td>\n",
       "      <td>[what, is, not, to, like]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>961291</th>\n",
       "      <td>5</td>\n",
       "      <td>excellent product at a great price</td>\n",
       "      <td>2</td>\n",
       "      <td>[excellent, product, at, great, price]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2079982</th>\n",
       "      <td>5</td>\n",
       "      <td>proscheap inkhassle free experiencefast shippi...</td>\n",
       "      <td>2</td>\n",
       "      <td>[proscheap, inkhassle, free, experiencefast, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200321</th>\n",
       "      <td>5</td>\n",
       "      <td>perfect we needed these for a trade show and t...</td>\n",
       "      <td>2</td>\n",
       "      <td>[perfect, we, needed, these, for, trade, show,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1028783</th>\n",
       "      <td>5</td>\n",
       "      <td>great for the cc cage i built for my guinea pi...</td>\n",
       "      <td>2</td>\n",
       "      <td>[great, for, the, cc, cage, built, for, my, gu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>757222</th>\n",
       "      <td>1</td>\n",
       "      <td>was very surpriseddid lots of research find th...</td>\n",
       "      <td>1</td>\n",
       "      <td>[was, very, surpriseddid, lots, of, research, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>802600</th>\n",
       "      <td>3</td>\n",
       "      <td>comes with all hardware to mount directly to o...</td>\n",
       "      <td>1</td>\n",
       "      <td>[comes, with, all, hardware, to, mount, direct...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1813173</th>\n",
       "      <td>2</td>\n",
       "      <td>great phone for skype use phone died about mon...</td>\n",
       "      <td>1</td>\n",
       "      <td>[great, phone, for, skype, use, phone, died, a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2602978</th>\n",
       "      <td>3</td>\n",
       "      <td>it is missing a critical feature there is no w...</td>\n",
       "      <td>1</td>\n",
       "      <td>[it, is, missing, critical, feature, there, is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>761848</th>\n",
       "      <td>2</td>\n",
       "      <td>i was very disappointed in this calendar my bi...</td>\n",
       "      <td>1</td>\n",
       "      <td>[was, very, disappointed, in, this, calendar, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        star_rating                                        review_body  class  \\\n",
       "595562            5                                what is not to like      2   \n",
       "961291            5                 excellent product at a great price      2   \n",
       "2079982           5  proscheap inkhassle free experiencefast shippi...      2   \n",
       "200321            5  perfect we needed these for a trade show and t...      2   \n",
       "1028783           5  great for the cc cage i built for my guinea pi...      2   \n",
       "...             ...                                                ...    ...   \n",
       "757222            1  was very surpriseddid lots of research find th...      1   \n",
       "802600            3  comes with all hardware to mount directly to o...      1   \n",
       "1813173           2  great phone for skype use phone died about mon...      1   \n",
       "2602978           3  it is missing a critical feature there is no w...      1   \n",
       "761848            2  i was very disappointed in this calendar my bi...      1   \n",
       "\n",
       "                                                     token  \n",
       "595562                           [what, is, not, to, like]  \n",
       "961291              [excellent, product, at, great, price]  \n",
       "2079982  [proscheap, inkhassle, free, experiencefast, s...  \n",
       "200321   [perfect, we, needed, these, for, trade, show,...  \n",
       "1028783  [great, for, the, cc, cage, built, for, my, gu...  \n",
       "...                                                    ...  \n",
       "757222   [was, very, surpriseddid, lots, of, research, ...  \n",
       "802600   [comes, with, all, hardware, to, mount, direct...  \n",
       "1813173  [great, phone, for, skype, use, phone, died, a...  \n",
       "2602978  [it, is, missing, critical, feature, there, is...  \n",
       "761848   [was, very, disappointed, in, this, calendar, ...  \n",
       "\n",
       "[100000 rows x 4 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "balanced_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "190ac253",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate embedings\n",
    "def avg_word2vect(token_list, model):\n",
    "    if len(token_list) < 1:\n",
    "        return np.zeros(300)\n",
    "    else:\n",
    "        vector = [model[word] if word in model else np.random.rand(300) for word in token_list]\n",
    "    averaged = np.mean(vector, axis=0)\n",
    "    return averaged\n",
    "embeddings = list(balanced_data['token'].apply(lambda x: avg_word2vect(x, word2vect_model)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "457e7b1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Queen', 0.4929388165473938)]\n",
      "[('mother', 0.7526409029960632)]\n",
      "[('Hindi', 0.530886709690094)]\n",
      "[('projectors', 0.6412938237190247)]\n"
     ]
    }
   ],
   "source": [
    "# Examples using Google's word2vect\n",
    "print(word2vect_model.most_similar(positive=['King', 'Woman'], negative=['Man'], topn=1))\n",
    "print(word2vect_model.most_similar(positive=['father', 'Woman'], negative=['Man'], topn=1))\n",
    "print(word2vect_model.most_similar(positive=['English', 'India'], negative=['US'], topn=1))\n",
    "print(word2vect_model.most_similar(positive=['printer', 'projector'], negative=['cartridge'], topn=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "19550393",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Word2Vec model\n",
    "custom_model = Word2Vec(\n",
    "    sentences=balanced_data['token'].tolist(), \n",
    "    vector_size=300, \n",
    "    window=13, \n",
    "    min_count=9, \n",
    "    workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e0246757",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('reflective', 0.5921333432197571)]\n",
      "[('sister', 0.6675557494163513)]\n",
      "[('language', 0.6544055342674255)]\n"
     ]
    }
   ],
   "source": [
    "# Examples using custom word2vect\n",
    "print(custom_model.wv.most_similar(positive=['king', 'woman'], negative=['man'], topn=1))\n",
    "print(custom_model.wv.most_similar(positive=['father', 'woman'], negative=['man'], topn=1))\n",
    "print(custom_model.wv.most_similar(positive=['english', 'india'], negative=['us'], topn=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1169094",
   "metadata": {},
   "source": [
    "The pretrained model is able to encode semantic similarities between words better than the custom model as the pretrained model gives outputs that are pretty close to the provided input words. The pretrained model seems to capture the correlations between words in a much efficient manner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9ca36b",
   "metadata": {},
   "source": [
    "### 3. Perceptron using Word2Vec and TF-IDF features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3479f61f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Accuracy for Perceptron using TFIDF features are\n",
      "0.8045\n",
      "The scores for Perceptron using Word2Vec features are\n",
      "0.7775\n"
     ]
    }
   ],
   "source": [
    "# TFIDF for perceptron\n",
    "X = balanced_data['review_body']\n",
    "Y = balanced_data['class']\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_X = tfidf_vectorizer.fit_transform(X)\n",
    "X_traintf_tfidf, X_testtf_tfidf, Y_traintf_tfidf, Y_testtf_tfidf = train_test_split(tfidf_X, Y, test_size=0.2)\n",
    "\n",
    "perceptron_tfidf = Perceptron()\n",
    "perceptron_tfidf.fit(X_traintf_tfidf, Y_traintf_tfidf.astype('int'))\n",
    "\n",
    "y_tf = perceptron_tfidf.predict(X_testtf_tfidf)\n",
    "\n",
    "\n",
    "accuracy_perceptron_tf = accuracy_score(Y_testtf_tfidf.astype('int'), y_tf.astype('int'))\n",
    "print(\"The Accuracy for Perceptron using TFIDF features are\")\n",
    "print(f\"{accuracy_perceptron_tf :.4f}\")\n",
    "\n",
    "# Word to vect for perceptron\n",
    "X_w2v = embeddings\n",
    "Y_w2v = balanced_data['class']\n",
    "\n",
    "X_traintf_w2v, X_testtf_w2v, Y_traintf_w2v, Y_testtf_w2v = train_test_split(X_w2v, Y_w2v, test_size=0.2, random_state=50)\n",
    "\n",
    "perceptron_w2v = Perceptron()\n",
    "perceptron_w2v.fit(X_traintf_w2v, Y_traintf_w2v.astype('int'))\n",
    "\n",
    "y_w2v = perceptron_w2v.predict(X_testtf_w2v)\n",
    "\n",
    "accuracy_perceptron_w2v = accuracy_score(Y_testtf_w2v.astype('int'), y_w2v.astype('int'))\n",
    "print(\"The scores for Perceptron using Word2Vec features are\")\n",
    "print(f\"{accuracy_perceptron_w2v :.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150f7346",
   "metadata": {},
   "source": [
    "### 3. SVM using Word2Vec and TF-IDF features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "798794e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The scores for SVM using TFIDF features are\n",
      "0.8554\n",
      "The scores for SVM using Word to vec features are\n",
      "0.7970\n"
     ]
    }
   ],
   "source": [
    "# TFIDF for SVM\n",
    "svm_model_tf = LinearSVC()\n",
    "svm_model_tf.fit(X_traintf_tfidf, Y_traintf_tfidf.astype('int'))\n",
    "\n",
    "y_tf_svm = svm_model_tf.predict(X_testtf_tfidf)\n",
    "\n",
    "accuracy_svm_tf = accuracy_score(Y_testtf_tfidf.astype('int'), y_tf_svm.astype('int'))\n",
    "print(\"The scores for SVM using TFIDF features are\")\n",
    "print(f\"{accuracy_svm_tf :.4f}\")\n",
    "\n",
    "# Word to vect for perceptron\n",
    "svm_model_w2v = LinearSVC()\n",
    "svm_model_w2v.fit(X_traintf_w2v, Y_traintf_w2v.astype('int'))\n",
    "y_w2v_svm = svm_model_w2v.predict(X_testtf_w2v)\n",
    "\n",
    "accuracy_svm_w2v = accuracy_score(Y_testtf_w2v.astype('int'), y_w2v_svm.astype('int'))\n",
    "print(\"The scores for SVM using Word to vec features are\")\n",
    "print(f\"{accuracy_svm_w2v :.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa80f4e4",
   "metadata": {},
   "source": [
    "From my implementation it can be concluded that the models using TF-IDF perform better resulting in better accuracies when compared to the models using Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00589b72",
   "metadata": {},
   "source": [
    "### 4. Feedforward Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d10a218",
   "metadata": {},
   "source": [
    "#### a) To generate the input features, use the average Word2Vec vectors similar to the “Simple models” section and train the neural network. Report accuracy values on the testing split for your MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "412ee5ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Loss: 0.3328\n",
      "Epoch 2/50, Loss: 0.3016\n",
      "Epoch 3/50, Loss: 0.2124\n",
      "Epoch 4/50, Loss: 0.1901\n",
      "Epoch 5/50, Loss: 0.3681\n",
      "Epoch 6/50, Loss: 0.4741\n",
      "Epoch 7/50, Loss: 0.4968\n",
      "Epoch 8/50, Loss: 0.5736\n",
      "Epoch 9/50, Loss: 0.4107\n",
      "Epoch 10/50, Loss: 0.3572\n",
      "Epoch 11/50, Loss: 0.3188\n",
      "Epoch 12/50, Loss: 0.3158\n",
      "Epoch 13/50, Loss: 0.1604\n",
      "Epoch 14/50, Loss: 0.4736\n",
      "Epoch 15/50, Loss: 0.4189\n",
      "Epoch 16/50, Loss: 0.5407\n",
      "Epoch 17/50, Loss: 0.5133\n",
      "Epoch 18/50, Loss: 0.4673\n",
      "Epoch 19/50, Loss: 0.3893\n",
      "Epoch 20/50, Loss: 0.3141\n",
      "Epoch 21/50, Loss: 0.1827\n",
      "Epoch 22/50, Loss: 0.5038\n",
      "Epoch 23/50, Loss: 0.3588\n",
      "Epoch 24/50, Loss: 0.4236\n",
      "Epoch 25/50, Loss: 0.3920\n",
      "Epoch 26/50, Loss: 0.4644\n",
      "Epoch 27/50, Loss: 0.4323\n",
      "Epoch 28/50, Loss: 0.1715\n",
      "Epoch 29/50, Loss: 0.5116\n",
      "Epoch 30/50, Loss: 0.4135\n",
      "Epoch 31/50, Loss: 0.3885\n",
      "Epoch 32/50, Loss: 0.3552\n",
      "Epoch 33/50, Loss: 0.3500\n",
      "Epoch 34/50, Loss: 0.5384\n",
      "Epoch 35/50, Loss: 0.4258\n",
      "Epoch 36/50, Loss: 0.3157\n",
      "Epoch 37/50, Loss: 0.2802\n",
      "Epoch 38/50, Loss: 0.3341\n",
      "Epoch 39/50, Loss: 0.4260\n",
      "Epoch 40/50, Loss: 0.2596\n",
      "Epoch 41/50, Loss: 0.3421\n",
      "Epoch 42/50, Loss: 0.3301\n",
      "Epoch 43/50, Loss: 0.3613\n",
      "Epoch 44/50, Loss: 0.2410\n",
      "Epoch 45/50, Loss: 0.3944\n",
      "Epoch 46/50, Loss: 0.3695\n",
      "Epoch 47/50, Loss: 0.2425\n",
      "Epoch 48/50, Loss: 0.2520\n",
      "Epoch 49/50, Loss: 0.2013\n",
      "Epoch 50/50, Loss: 0.3831\n",
      "Accuracy on test set: 79.42%\n"
     ]
    }
   ],
   "source": [
    "X = torch.tensor(embeddings)\n",
    "Y = torch.tensor(balanced_data['class'].values.astype(np.int64)).long() - 1\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y,test_size=0.2)\n",
    "\n",
    "# An abstract class representing a Dataset.\n",
    "class AmazonDataset(Dataset):\n",
    "    def __init__(self, X, Y):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.Y)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.X[index], self.Y[index]\n",
    "    \n",
    "train_data = AmazonDataset(X_train, Y_train)\n",
    "test_data = AmazonDataset(X_test, Y_test)\n",
    "    \n",
    "    \n",
    "# number of subprocesses to use for data loading\n",
    "num_workers = 0\n",
    "# how many samples per batch to load\n",
    "batch_size = 20\n",
    "# percentage of training set to use as validation\n",
    "valid_size = 0.2\n",
    "\n",
    "# prepare data loaders\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, num_workers=num_workers, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, num_workers=num_workers, shuffle=False)\n",
    "\n",
    "# define the FNN architecture\n",
    "class FNN(nn.Module):\n",
    "    def __init__(self, dimension):\n",
    "        super(FNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(dimension, 50)\n",
    "        self.fc2 = nn.Linear(50, 5)\n",
    "        self.fc3 = nn.Linear(5, 2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "model = FNN(300)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 50\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data.float())\n",
    "        loss = criterion(outputs, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "#Evaluate the Model\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "prediction_list = []\n",
    "with torch.no_grad():\n",
    "    for data, target in test_loader:\n",
    "        outputs = model(data.float())\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        prediction_list.extend(list(np.array(predicted.cpu())))\n",
    "\n",
    "print(f\"Accuracy on test set: {100 * accuracy_score(Y_test, prediction_list)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aa39869a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Loss: 0.5847\n",
      "Epoch 2/50, Loss: 0.3547\n",
      "Epoch 3/50, Loss: 0.5817\n",
      "Epoch 4/50, Loss: 0.4619\n",
      "Epoch 5/50, Loss: 0.2506\n",
      "Epoch 6/50, Loss: 0.3506\n",
      "Epoch 7/50, Loss: 0.3388\n",
      "Epoch 8/50, Loss: 0.3736\n",
      "Epoch 9/50, Loss: 0.2418\n",
      "Epoch 10/50, Loss: 0.3006\n",
      "Epoch 11/50, Loss: 0.3013\n",
      "Epoch 12/50, Loss: 0.1590\n",
      "Epoch 13/50, Loss: 0.0686\n",
      "Epoch 14/50, Loss: 0.2021\n",
      "Epoch 15/50, Loss: 0.0719\n",
      "Epoch 16/50, Loss: 0.2718\n",
      "Epoch 17/50, Loss: 0.0773\n",
      "Epoch 18/50, Loss: 0.3071\n",
      "Epoch 19/50, Loss: 0.1482\n",
      "Epoch 20/50, Loss: 0.3042\n",
      "Epoch 21/50, Loss: 0.0282\n",
      "Epoch 22/50, Loss: 0.1709\n",
      "Epoch 23/50, Loss: 0.2143\n",
      "Epoch 24/50, Loss: 0.1521\n",
      "Epoch 25/50, Loss: 0.2340\n",
      "Epoch 26/50, Loss: 0.1867\n",
      "Epoch 27/50, Loss: 0.0677\n",
      "Epoch 28/50, Loss: 0.0172\n",
      "Epoch 29/50, Loss: 0.0328\n",
      "Epoch 30/50, Loss: 0.0393\n",
      "Epoch 31/50, Loss: 0.1344\n",
      "Epoch 32/50, Loss: 0.0623\n",
      "Epoch 33/50, Loss: 0.1307\n",
      "Epoch 34/50, Loss: 0.1306\n",
      "Epoch 35/50, Loss: 0.0309\n",
      "Epoch 36/50, Loss: 0.0057\n",
      "Epoch 37/50, Loss: 0.0147\n",
      "Epoch 38/50, Loss: 0.0087\n",
      "Epoch 39/50, Loss: 0.1367\n",
      "Epoch 40/50, Loss: 0.0292\n",
      "Epoch 41/50, Loss: 0.1408\n",
      "Epoch 42/50, Loss: 0.1814\n",
      "Epoch 43/50, Loss: 0.0118\n",
      "Epoch 44/50, Loss: 0.0174\n",
      "Epoch 45/50, Loss: 0.0217\n",
      "Epoch 46/50, Loss: 0.0765\n",
      "Epoch 47/50, Loss: 0.0026\n",
      "Epoch 48/50, Loss: 0.1557\n",
      "Epoch 49/50, Loss: 0.0351\n",
      "Epoch 50/50, Loss: 0.0029\n",
      "Accuracy on test set: 70.855%\n"
     ]
    }
   ],
   "source": [
    "def concatenate_word2vec(token_list, model, num_words=10):\n",
    "    # Initialize a list to hold vectors\n",
    "    vectors = []\n",
    "    # Iterate through the tokens in the list\n",
    "    for token in token_list[:num_words]:\n",
    "        # Check if the token exists in the Word2Vec model\n",
    "        if token in model:\n",
    "            vectors.append(model[token])\n",
    "        else:\n",
    "            vectors.append(np.random.rand(300))  \n",
    "    \n",
    "    # If the review has less than num_words, pad it with zero vectors\n",
    "    while len(vectors) < num_words:\n",
    "        vectors.append(np.zeros(300))\n",
    "    \n",
    "    # Concatenate the vectors\n",
    "    concatenated_vector = np.concatenate(vectors)\n",
    "    \n",
    "    return concatenated_vector\n",
    "\n",
    "# Generate the concatenated embeddings for each review\n",
    "concatenated_embeddings = list(balanced_data['token'].apply(lambda x: concatenate_word2vec(x, word2vect_model)))\n",
    "\n",
    "X = torch.tensor(concatenated_embeddings)\n",
    "Y = torch.tensor(balanced_data['class'].values.astype(np.int64)).long() - 1\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y,test_size=0.2)\n",
    "    \n",
    "train_data = AmazonDataset(X_train, Y_train)\n",
    "test_data = AmazonDataset(X_test, Y_test)\n",
    "    \n",
    "    \n",
    "# number of subprocesses to use for data loading\n",
    "num_workers = 0\n",
    "# how many samples per batch to load\n",
    "batch_size = 20\n",
    "# percentage of training set to use as validation\n",
    "valid_size = 0.2\n",
    "\n",
    "# prepare data loaders\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, num_workers=num_workers, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, num_workers=num_workers, shuffle=False)\n",
    "\n",
    "# define the FNN architecture\n",
    "class FNN(nn.Module):\n",
    "    def __init__(self, dimension):\n",
    "        super(FNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(dimension, 50)\n",
    "        self.fc2 = nn.Linear(50, 5)\n",
    "        self.fc3 = nn.Linear(5, 2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "model = FNN(3000)\n",
    "# specify loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# specify optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 50\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data.float())\n",
    "        loss = criterion(outputs, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Evaluate the Model\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "prediction_list = []\n",
    "with torch.no_grad():\n",
    "    for data, target in test_loader:\n",
    "        outputs = model(data.float())\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        prediction_list.extend(list(np.array(predicted.cpu())))\n",
    "\n",
    "print(f\"Accuracy on test set: {100 * accuracy_score(Y_test, prediction_list)}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b41adde3",
   "metadata": {},
   "source": [
    "From the accuracies obtained for the feedforward neural networks it can be concluded that Simple models performed better with the Word2Vec embeddings that I generated. This might be due to the data that is being used "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee2f083",
   "metadata": {},
   "source": [
    "### 5. Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3940b08",
   "metadata": {},
   "source": [
    "#### a) Train a simple RNN for sentiment analysis. You can consider an RNN cell with the hidden state size of 10. To feed your data into our RNN, limit the maximum review length to 10 by truncating longer reviews and padding shorter reviews with a null value (0). Report accuracy values on the testing split for your RNN model. What do you conclude by comparing accuracy values you obtain with those obtained with feedforward neural network models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fbf5c9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequence(token_list, model):\n",
    "    rnn_vectors = []\n",
    "    for token in token_list[:10]:\n",
    "        if token in model:\n",
    "            rnn_vectors.append(model[token])\n",
    "        else:\n",
    "            rnn_vectors.append(np.zeros(300))\n",
    "    \n",
    "    while len(rnn_vectors) < 10:\n",
    "        rnn_vectors.append(np.zeros(300))\n",
    "    \n",
    "    return rnn_vectors\n",
    "\n",
    "\n",
    "rnn_sequences = balanced_data['token'].apply(lambda x: prepare_sequence(x, word2vect_model))\n",
    "\n",
    "X_rnn = torch.tensor(np.array(rnn_sequences.tolist())) \n",
    "Y_rnn = torch.tensor(balanced_data['class'].values).long() - 1\n",
    "\n",
    "X_train_rnn, X_test_rnn, Y_train_rnn, Y_test_rnn = train_test_split(X_rnn, Y_rnn, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b161dc6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Loss: 0.5162\n",
      "Epoch 2/50, Loss: 0.4079\n",
      "Epoch 3/50, Loss: 0.4128\n",
      "Epoch 4/50, Loss: 0.3414\n",
      "Epoch 5/50, Loss: 0.5033\n",
      "Epoch 6/50, Loss: 0.4499\n",
      "Epoch 7/50, Loss: 0.3953\n",
      "Epoch 8/50, Loss: 0.5981\n",
      "Epoch 9/50, Loss: 0.4769\n",
      "Epoch 10/50, Loss: 0.4125\n",
      "Epoch 11/50, Loss: 0.3873\n",
      "Epoch 12/50, Loss: 0.5468\n",
      "Epoch 13/50, Loss: 0.3387\n",
      "Epoch 14/50, Loss: 0.6357\n",
      "Epoch 15/50, Loss: 0.4404\n",
      "Epoch 16/50, Loss: 0.5140\n",
      "Epoch 17/50, Loss: 0.3417\n",
      "Epoch 18/50, Loss: 0.2846\n",
      "Epoch 19/50, Loss: 0.6856\n",
      "Epoch 20/50, Loss: 0.3207\n",
      "Epoch 21/50, Loss: 0.5669\n",
      "Epoch 22/50, Loss: 0.5033\n",
      "Epoch 23/50, Loss: 0.5582\n",
      "Epoch 24/50, Loss: 0.2831\n",
      "Epoch 25/50, Loss: 0.5615\n",
      "Epoch 26/50, Loss: 0.3843\n",
      "Epoch 27/50, Loss: 0.4704\n",
      "Epoch 28/50, Loss: 0.4151\n",
      "Epoch 29/50, Loss: 0.3823\n",
      "Epoch 30/50, Loss: 0.4371\n",
      "Epoch 31/50, Loss: 0.5365\n",
      "Epoch 32/50, Loss: 0.3994\n",
      "Epoch 33/50, Loss: 0.5854\n",
      "Epoch 34/50, Loss: 0.3706\n",
      "Epoch 35/50, Loss: 0.4447\n",
      "Epoch 36/50, Loss: 0.6450\n",
      "Epoch 37/50, Loss: 0.3674\n",
      "Epoch 38/50, Loss: 0.5307\n",
      "Epoch 39/50, Loss: 0.4645\n",
      "Epoch 40/50, Loss: 0.3483\n",
      "Epoch 41/50, Loss: 0.4117\n",
      "Epoch 42/50, Loss: 0.3125\n",
      "Epoch 43/50, Loss: 0.7319\n",
      "Epoch 44/50, Loss: 0.4846\n",
      "Epoch 45/50, Loss: 0.7604\n",
      "Epoch 46/50, Loss: 0.2036\n",
      "Epoch 47/50, Loss: 0.3158\n",
      "Epoch 48/50, Loss: 0.3314\n",
      "Epoch 49/50, Loss: 0.3841\n",
      "Epoch 50/50, Loss: 0.3395\n",
      "Accuracy on test set: 77.105%\n"
     ]
    }
   ],
   "source": [
    "X = torch.tensor(embeddings)\n",
    "Y = torch.tensor(balanced_data['class'].values.astype(np.int64)).long() - 1\n",
    "\n",
    "# Define RNN architechture\n",
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim, output_dim):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.rnn = nn.RNN(dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        _, h_n = self.rnn(x)\n",
    "        output = self.fc(h_n.squeeze(0))\n",
    "        return output\n",
    "\n",
    "rnn_model = RNNModel(300, 10, 2)\n",
    "\n",
    "# specify loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# specify optimizer\n",
    "optimizer = torch.optim.Adam(rnn_model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 50\n",
    "# number of subprocesses to use for data loading\n",
    "num_workers = 0\n",
    "# how many samples per batch to load\n",
    "batch_size = 20\n",
    "# percentage of training set to use as validation\n",
    "valid_size = 0.2\n",
    "\n",
    "train_dataset_rnn = AmazonDataset(X_train_rnn, Y_train_rnn)\n",
    "train_loader_rnn = DataLoader(train_dataset_rnn, batch_size=batch_size, num_workers=num_workers, shuffle=True)\n",
    "test_dataset_rnn = AmazonDataset(X_test_rnn, Y_test_rnn)\n",
    "test_loader_rnn = DataLoader(test_dataset_rnn, batch_size=batch_size, num_workers=num_workers, shuffle=False)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    rnn_model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader_rnn):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = rnn_model(data.float())\n",
    "        loss = criterion(outputs, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Evaluate the Model\n",
    "rnn_model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "prediction_list = []\n",
    "with torch.no_grad():\n",
    "    for data, target in test_loader_rnn:\n",
    "        outputs = rnn_model(data.float())\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        prediction_list.extend(list(np.array(predicted.cpu())))\n",
    "\n",
    "print(f\"Accuracy on test set: {100 * accuracy_score(Y_test_rnn, prediction_list)}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "359445d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([20, 2])\n",
      "Target shape: torch.Size([20])\n"
     ]
    }
   ],
   "source": [
    "print(\"Output shape:\", outputs.shape)\n",
    "print(\"Target shape:\", target.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5162e399",
   "metadata": {},
   "source": [
    "#### b) Repeat part (a) by considering a gated recurrent unit cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ece4cd6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Loss: 0.7548\n",
      "Epoch 2/50, Loss: 0.3663\n",
      "Epoch 3/50, Loss: 0.4254\n",
      "Epoch 4/50, Loss: 0.3061\n",
      "Epoch 5/50, Loss: 0.6919\n",
      "Epoch 6/50, Loss: 0.4189\n",
      "Epoch 7/50, Loss: 0.1438\n",
      "Epoch 8/50, Loss: 0.5460\n",
      "Epoch 9/50, Loss: 0.5215\n",
      "Epoch 10/50, Loss: 0.4524\n",
      "Epoch 11/50, Loss: 0.3290\n",
      "Epoch 12/50, Loss: 0.4110\n",
      "Epoch 13/50, Loss: 0.5167\n",
      "Epoch 14/50, Loss: 0.4712\n",
      "Epoch 15/50, Loss: 0.3478\n",
      "Epoch 16/50, Loss: 0.4748\n",
      "Epoch 17/50, Loss: 0.2202\n",
      "Epoch 18/50, Loss: 0.5135\n",
      "Epoch 19/50, Loss: 0.3056\n",
      "Epoch 20/50, Loss: 0.6678\n",
      "Epoch 21/50, Loss: 0.4572\n",
      "Epoch 22/50, Loss: 0.4655\n",
      "Epoch 23/50, Loss: 0.4044\n",
      "Epoch 24/50, Loss: 0.3621\n",
      "Epoch 25/50, Loss: 0.4841\n",
      "Epoch 26/50, Loss: 0.3916\n",
      "Epoch 27/50, Loss: 0.3763\n",
      "Epoch 28/50, Loss: 0.3124\n",
      "Epoch 29/50, Loss: 0.4728\n",
      "Epoch 30/50, Loss: 0.3608\n",
      "Epoch 31/50, Loss: 0.4319\n",
      "Epoch 32/50, Loss: 0.3058\n",
      "Epoch 33/50, Loss: 0.4074\n",
      "Epoch 34/50, Loss: 0.3467\n",
      "Epoch 35/50, Loss: 0.6184\n",
      "Epoch 36/50, Loss: 0.3947\n",
      "Epoch 37/50, Loss: 0.2309\n",
      "Epoch 38/50, Loss: 0.2335\n",
      "Epoch 39/50, Loss: 0.2872\n",
      "Epoch 40/50, Loss: 0.2371\n",
      "Epoch 41/50, Loss: 0.4297\n",
      "Epoch 42/50, Loss: 0.1880\n",
      "Epoch 43/50, Loss: 0.2323\n",
      "Epoch 44/50, Loss: 0.3533\n",
      "Epoch 45/50, Loss: 0.1678\n",
      "Epoch 46/50, Loss: 0.3348\n",
      "Epoch 47/50, Loss: 0.3884\n",
      "Epoch 48/50, Loss: 0.3831\n",
      "Epoch 49/50, Loss: 0.1695\n",
      "Epoch 50/50, Loss: 0.5356\n",
      "Accuracy on test set: 78.14999999999999%\n"
     ]
    }
   ],
   "source": [
    "# specify GRU architecture\n",
    "class GRU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GRU, self).__init__()\n",
    "        self.gru = nn.GRU(300, 10, batch_first=True)\n",
    "        self.fc = nn.Linear(10,2)\n",
    "\n",
    "    def forward(self, x):     \n",
    "        outputs, hidden = self.gru(x)\n",
    "        out = self.fc(hidden.squeeze(0))\n",
    "        return out\n",
    "    \n",
    "gru_model = GRU()\n",
    "\n",
    "# Define loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# Define optimizer\n",
    "optimizer = torch.optim.Adam(gru_model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    gru_model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader_rnn):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = gru_model(data.float())\n",
    "        loss = criterion(outputs, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Evaluate the Model\n",
    "rnn_model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "prediction_list = []\n",
    "with torch.no_grad():\n",
    "    for data, target in test_loader_rnn:\n",
    "        outputs = gru_model(data.float())\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        prediction_list.extend(list(np.array(predicted.cpu())))\n",
    "\n",
    "print(f\"Accuracy on test set: {100 * accuracy_score(Y_test_rnn, prediction_list)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d7c510",
   "metadata": {},
   "source": [
    "#### Repeat part (a) by considering an LSTM unit cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "64adf8f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Loss: 0.5085\n",
      "Epoch 2/50, Loss: 0.4011\n",
      "Epoch 3/50, Loss: 0.5107\n",
      "Epoch 4/50, Loss: 0.2903\n",
      "Epoch 5/50, Loss: 0.5231\n",
      "Epoch 6/50, Loss: 0.5736\n",
      "Epoch 7/50, Loss: 0.3478\n",
      "Epoch 8/50, Loss: 0.3532\n",
      "Epoch 9/50, Loss: 0.3659\n",
      "Epoch 10/50, Loss: 0.2765\n",
      "Epoch 11/50, Loss: 0.6873\n",
      "Epoch 12/50, Loss: 0.6251\n",
      "Epoch 13/50, Loss: 0.4043\n",
      "Epoch 14/50, Loss: 0.2225\n",
      "Epoch 15/50, Loss: 0.5057\n",
      "Epoch 16/50, Loss: 0.3865\n",
      "Epoch 17/50, Loss: 0.7284\n",
      "Epoch 18/50, Loss: 0.3600\n",
      "Epoch 19/50, Loss: 0.2412\n",
      "Epoch 20/50, Loss: 0.5965\n",
      "Epoch 21/50, Loss: 0.5539\n",
      "Epoch 22/50, Loss: 0.4326\n",
      "Epoch 23/50, Loss: 0.3650\n",
      "Epoch 24/50, Loss: 0.3370\n",
      "Epoch 25/50, Loss: 0.5511\n",
      "Epoch 26/50, Loss: 0.3249\n",
      "Epoch 27/50, Loss: 0.1773\n",
      "Epoch 28/50, Loss: 0.3519\n",
      "Epoch 29/50, Loss: 0.4769\n",
      "Epoch 30/50, Loss: 0.3361\n",
      "Epoch 31/50, Loss: 0.2924\n",
      "Epoch 32/50, Loss: 0.1658\n",
      "Epoch 33/50, Loss: 0.1995\n",
      "Epoch 34/50, Loss: 0.2678\n",
      "Epoch 35/50, Loss: 0.3836\n",
      "Epoch 36/50, Loss: 0.2170\n",
      "Epoch 37/50, Loss: 0.2131\n",
      "Epoch 38/50, Loss: 0.4101\n",
      "Epoch 39/50, Loss: 0.5496\n",
      "Epoch 40/50, Loss: 0.1825\n",
      "Epoch 41/50, Loss: 0.5015\n",
      "Epoch 42/50, Loss: 0.4948\n",
      "Epoch 43/50, Loss: 0.2533\n",
      "Epoch 44/50, Loss: 0.4823\n",
      "Epoch 45/50, Loss: 0.4412\n",
      "Epoch 46/50, Loss: 0.2388\n",
      "Epoch 47/50, Loss: 0.1991\n",
      "Epoch 48/50, Loss: 0.3819\n",
      "Epoch 49/50, Loss: 0.4203\n",
      "Epoch 50/50, Loss: 0.2983\n",
      "Accuracy on test set: 77.535%\n"
     ]
    }
   ],
   "source": [
    "# Define LSTM architecture\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LSTM, self).__init__()\n",
    "        \n",
    "        # Define the LSTM layer\n",
    "        self.lstm = nn.LSTM(300,10, batch_first=True)\n",
    "        \n",
    "        # Define the output layer\n",
    "        self.linear = nn.Linear(10, 2)\n",
    "        \n",
    "    def forward(self, text):\n",
    "        packed_output, _ = self.lstm(text)\n",
    "        out = self.linear(packed_output[:, -1, :])\n",
    "        return out\n",
    "\n",
    "lstm_model = LSTM()\n",
    "\n",
    "# Define loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# Define optimizers\n",
    "optimizer = torch.optim.Adam(lstm_model.parameters())\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    lstm_model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader_rnn):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = lstm_model(data.float())\n",
    "        loss = criterion(outputs, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Evaluate the Model\n",
    "lstm_model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "prediction_list = []\n",
    "with torch.no_grad():\n",
    "    for data, target in test_loader_rnn:\n",
    "        outputs = lstm_model(data.float())\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        prediction_list.extend(list(np.array(predicted.cpu())))\n",
    "\n",
    "print(f\"Accuracy on test set: {100 * accuracy_score(Y_test_rnn, prediction_list)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7ce16d",
   "metadata": {},
   "source": [
    "By comparing accuracy values obtained by GRU, LSTM, and simple RNN it can be concluded that for the given embeddings GRU & LSTM perform slightly better than RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b9a0b4",
   "metadata": {},
   "source": [
    "### References:\n",
    "https://www.kaggle.com/code/mishra1993/pytorch-multi-layer-perceptron-mnist/notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd3da73",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
